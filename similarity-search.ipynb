{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed6eb2d-bd5d-41f3-a733-fb687f7f4290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-23 06:58:18,464 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://snurran.devnet.hops.works/p/120\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import hopsworks\n",
    "import os\n",
    "\n",
    "proj = hopsworks.login()\n",
    "fs = proj.get_feature_store()\n",
    "mr = proj.get_model_registry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4edcab63-fb1a-44af-b243-3d177d2d4ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from Model Registry\n",
      "Downloading model artifact (0 dirs, 9 files)... DONE\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "name=\"openaiclip_vit_base_patch32\"\n",
    "model_mr = mr.get_model(name, version=1)\n",
    "if model_mr is None:\n",
    "    print(\"Downloading model from HF\")\n",
    "    model_name = \"openai/clip-vit-base-patch32\"\n",
    "    model = CLIPModel.from_pretrained(model_name)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)    \n",
    "    save_dir = \"/tmp/clip-vit-base-patch32-local\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.save_pretrained(save_dir)\n",
    "    processor.save_pretrained(save_dir)\n",
    "    model_mr =  mr.python.create_model(\n",
    "        name=name, \n",
    "        description=\"Image vector embedding model from OpenAI\",\n",
    "    )\n",
    "    model_mr.save(save_dir)\n",
    "else:\n",
    "    print(\"Downloading model from Model Registry\")\n",
    "    model_mr.download(local_path=\"/tmp\")\n",
    "    local_path = \"/tmp\"\n",
    "    model = CLIPModel.from_pretrained(local_path)\n",
    "    processor = CLIPProcessor.from_pretrained(local_path)\n",
    "\n",
    "# Move model to GPU (only if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd340e9-a6e7-4e7e-9798-c3fbca12f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = model.get_image_features(**inputs)\n",
    "\n",
    "        # Normalize\n",
    "        embedding = embedding / embedding.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        # Move back to CPU before converting to list\n",
    "        return embedding.squeeze().cpu().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad6f6a-c803-4cd1-b1db-f9042636d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg =  fs.get_feature_group(\"wider_face_files\", version=1)\n",
    "df = fg.read()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416c415-3bfa-4dc3-b936-7289253e9e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[['file_path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfa0e2-4fef-42ca-95c3-ba93a6ac2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda76fb5-503e-4391-8021-d0475e077507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# # Apply function to each row\n",
    "df1[\"embedding\"] = df1[\"file_path\"].apply(get_image_embedding)\n",
    "\n",
    "print(f\"Execution time: {time.time() - start:.4f} seconds\")\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa35f0-aef0-44ad-84ee-52a6edb948e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hopsworks import hsfs\n",
    "embedding_index = hsfs.embedding.EmbeddingIndex()\n",
    "embedding_index.add_embedding(\"embedding\", dimension=model.config.projection_dim, model=model_mr)\n",
    "\n",
    "# Create or get feature group\n",
    "fg = fs.get_or_create_feature_group(\n",
    "    name=\"image_embeddings\",\n",
    "    version=1,\n",
    "    primary_key=[\"file_path\"],\n",
    "    online_enabled=True,\n",
    "    description=\"image embeddings\",\n",
    "    embedding_index=embedding_index,    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68eda1e-5a72-4d70-a764-826d68994cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg.insert(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "266dabed-a26d-4be6-99b1-059f096794fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predict_similar_images.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predict_similar_images.py\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import hopsworks\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "from typing import Dict\n",
    "import kserve\n",
    "\n",
    "def get_image_embedding(image: Image.Image, processor, model):\n",
    "    try:\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model.get_image_features(**inputs)\n",
    "        embedding = embedding / embedding.norm(p=2, dim=-1, keepdim=True)\n",
    "        return embedding.squeeze().cpu().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "class ImageEmbeddingPredictor(kserve.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name)\n",
    "        self.image_dir = \"/hopsfs/Jupyter/yolov8-face/data/widerface/train\"\n",
    "        self.ready = True\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.fs = hopsworks.login().get_feature_store()\n",
    "        self.fg = fs.get_feature_group(\"image_embeddings\", version=1)\n",
    "        self.model = CLIPModel.from_pretrained(os.environ[\"MODEL_FILES_PATH\"])\n",
    "        self.processor = CLIPProcessor.from_pretrained(os.environ[\"MODEL_FILES_PATH\"])\n",
    "        self.model_mr = self.mr.get_model(\"openaiclip_vit_base_patch32\", version=1)\n",
    "        print(\"Initialization Complete\")\n",
    "    \n",
    "\n",
    "    def predict(self, request: Dict, headers: Dict[str, str] = None) -> Dict:\n",
    "        try:\n",
    "            # Get base64 image string from request\n",
    "            b64_image = request[\"image\"]\n",
    "            image_bytes = base64.b64decode(b64_image)\n",
    "            image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "            # Compute embedding\n",
    "            embedding = get_image_embedding(image, self.processor, self.model)\n",
    "\n",
    "            results = self.fg.find_neighbors( embedding, k=3)\n",
    "\n",
    "            for result in results:\n",
    "                print(result[1][0])\n",
    "                img = Image.open(result[1][0])\n",
    "                display(img)\n",
    "            \n",
    "            returned_images = []\n",
    "            returned_files = []\n",
    "            for result in results:\n",
    "                with open(result[1][0], \"rb\") as f:\n",
    "                    encoded = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "                    returned_images.append(encoded)\n",
    "                    returned_files.append(result[1][0])\n",
    "            return {\n",
    "                \"file_names\": returned_files,\n",
    "                \"images\": returned_images,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ca45b5-590d-4998-af7d-110cf0637009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8511b36edd234f9ca9e0d63a02d68330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading /hopsfs/Jupyter/yolov8-face/predict_similar_images.py: 0.000%|          | 0/2510 elapsed<00:00 remai…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment with the same name already exists. Getting existing deployment...\n",
      "To create a new deployment choose a different name.\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset API from the project\n",
    "dataset_api = proj.get_dataset_api()\n",
    "\n",
    "# Specify the file to upload (\"predict_example.py\") to the \"Models\" directory, and allow overwriting\n",
    "uploaded_file_path = dataset_api.upload(\"predict_similar_images.py\", model_mr.model_files_path, overwrite=True)\n",
    "\n",
    "# Construct the full path to the uploaded predictor script\n",
    "predictor_script_path = os.path.join(\"/Projects\", proj.name, uploaded_file_path)\n",
    "\n",
    "# Deploy the fraud model\n",
    "deployment = model_mr.deploy(\n",
    "    name=\"similarimages\",\n",
    "    script_file=predictor_script_path, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff380e4d-28f6-457b-894e-03c57b8f428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeba470-a791-404f-b6bf-892309409842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import requests\n",
    "import base64\n",
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "# Set your KServe endpoint here\n",
    "KSERVE_URL = \"http://<KSERVE_ENDPOINT>/v1/models/image-embedder:predict\"\n",
    "\n",
    "st.title(\"🔍 Similar Image Finder\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Show the uploaded image\n",
    "    image = Image.open(uploaded_file).convert(\"RGB\")\n",
    "    st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "\n",
    "    # Convert image to base64\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    b64_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    if st.button(\"Find Similar Images\"):\n",
    "        with st.spinner(\"Querying KServe model...\"):\n",
    "            payload = {\"image\": b64_image}\n",
    "            response = requests.post(KSERVE_URL, json=payload)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                if \"error\" in result:\n",
    "                    st.error(result[\"error\"])\n",
    "                else:\n",
    "                    st.success(\"Found similar images:\")\n",
    "                    for fname, b64_img in zip(result.get(\"file_names\", []), result.get(\"images\", [])):\n",
    "                        img_bytes = base64.b64decode(b64_img)\n",
    "                        sim_img = Image.open(io.BytesIO(img_bytes))\n",
    "                        st.image(sim_img, caption=fname, use_column_width=True)\n",
    "            else:\n",
    "                st.error(f\"Request failed: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
